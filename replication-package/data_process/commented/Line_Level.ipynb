{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b8bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scripts information:\n",
    "#Input: File level dataset. Files: \"Nova.csv\",\"Ironic.csv\", \"Base.csv\"\n",
    "#Output: File level result. File: \"csv_commented_lineLevel_raw.csv\" includes 'dataset','changeId','fileName','lineNumber','token','ngramScore','groundTruth','lengthScore'\n",
    "#                                  \"csv_commented_limescore.csv\" includes 'dataset','changeId','fileName','lineNumber','token','limeScore'\n",
    "#Description: This script is used to generate the csv files for predicting lines to be commented for RQ2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from scipy import sparse\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import time, pickle, math, warnings, os, operator\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# path that saves prediction result from n-gram model \n",
    "eval_file_path = './dataset/eval_file'\n",
    "\n",
    "# path that saves all trained models\n",
    "model_path = './dataset/ml-model'\n",
    "\n",
    "# path that saves all LIME models\n",
    "explainer_model_path = './dataset/lime-feature-model'\n",
    "\n",
    "#load file level dataset\n",
    "data_nova = pd.read_csv('./dataset/fileLevel/OpenStack_ex.csv', dtype=None, sep=',').to_numpy()\n",
    "# data_ironic = pd.read_csv('./dataset/fileLevel/Ironic.csv', dtype=None, sep=',').to_numpy()\n",
    "data_base = pd.read_csv('./dataset/fileLevel/Base_ex.csv', dtype=None, sep=',').to_numpy()\n",
    "#load line level dataset\n",
    "line_data_nova = pd.read_csv('./dataset/lineLevel/dataset-openstack-linelevel/OpenStack_ex.csv', dtype=None, sep=',').to_numpy()\n",
    "# line_data_ironic = pd.read_csv('./dataset/lineLevel/dataset-openstack-linelevel/Ironic.csv', dtype=None, sep=',').to_numpy()\n",
    "line_data_base = pd.read_csv('./dataset/lineLevel/dataset-qt-linelevel/qt_base_ex.csv', dtype=None, sep=',').to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b30675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate training/test datset\n",
    "def getDatasetFromRawData(project_source,data, bias):\n",
    "    row_data = data[0:,3]\n",
    "    row_data_Y = data[0:,0]\n",
    "    if project_source == \"base\":\n",
    "        row_data_deletions = data[0:,6]\n",
    "        row_data_additions = data[0:,7]\n",
    "        row_data_changedLine = data[0:,8]\n",
    "    else:\n",
    "        row_data_deletions = data[0:,7]\n",
    "        row_data_additions = data[0:,8]\n",
    "        row_data_changedLine = data[0:,9]\n",
    "    Y_train = []\n",
    "    is_comment = 0\n",
    "    not_comment = 0\n",
    "    for element in row_data_Y:\n",
    "        if(element == 0):\n",
    "            Y_train.append(False)\n",
    "            not_comment += 1\n",
    "        else:\n",
    "            Y_train.append(True)\n",
    "            is_comment += 1\n",
    "    Y_train = np.array(Y_train)\n",
    "\n",
    "    #finding a index that wouldn't separate file in same changeId into both training dataset and test dataset\n",
    "    first = int(len(data)*0.6)\n",
    "    divider = int(len(data)*0.2) + first + bias\n",
    "    data_count_vect = CountVectorizer(min_df=2, max_df=0.5)\n",
    "    train_row_data = row_data[:divider]\n",
    "    test_row_data = row_data[divider:]\n",
    "    train_row_data_deletions = row_data_deletions[:divider]\n",
    "    test_row_data_deletions = row_data_deletions[divider:]\n",
    "    train_row_data_additions = row_data_additions[:divider]\n",
    "    test_row_data_additions = row_data_additions[divider:]\n",
    "    train_row_data_changedLine = row_data_changedLine[:divider]\n",
    "    test_row_data_changedLine = row_data_changedLine[divider:]\n",
    "    data_train_counts = data_count_vect.fit_transform(train_row_data)\n",
    "    data_test_counts = data_count_vect.transform(test_row_data)\n",
    "    final_train_X = np.hstack((data_train_counts.toarray(),train_row_data_deletions[:,None]))\n",
    "    final_train_X = np.hstack((final_train_X,train_row_data_additions[:,None]))\n",
    "    final_train_X = np.hstack((final_train_X,train_row_data_changedLine[:,None]))\n",
    "    final_test_X = np.hstack((data_test_counts.toarray(),test_row_data_deletions[:,None]))\n",
    "    final_test_X = np.hstack((final_test_X,test_row_data_additions[:,None]))\n",
    "    final_test_X = np.hstack((final_test_X,test_row_data_changedLine[:,None]))\n",
    "    final_train_y = Y_train[:divider]\n",
    "    final_test_y = Y_train[divider:]\n",
    "    print(type(data_count_vect))\n",
    "    data_count_vect.fit_transform(train_row_data)\n",
    "    print(type(data_count_vect))\n",
    "    del data_train_counts,data_test_counts,train_row_data,test_row_data,data,row_data,row_data_Y\n",
    "    return final_train_X,final_train_y,final_test_X,final_test_y,divider,data_count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f784d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResult(x, y, model):\n",
    "    print(\"AUC:\",roc_auc_score(y, model.predict_proba(x)[:,1]))\n",
    "    print(\"Precision:\",precision_score(y, model.predict(x)))\n",
    "    print(\"Recall:\",recall_score(y, model.predict(x)))\n",
    "    print(\"F1:\",f1_score(y, model.predict(x)))\n",
    "    print(\"Confusion matrix: \\n\",confusion_matrix(y, model.predict(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989a2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRFmodel(project,rf_train_X,rf_train_y,rf_test_X,rf_test_y,seed,bias):\n",
    "    train_rf_model_path = model_path+'/smote_abstr_number_df_2_rf_'+project+'-'+str(seed)+str(bias)+'.pkl'\n",
    "    if not os.path.exists(train_rf_model_path):\n",
    "        rf = RandomForestClassifier(n_estimators=200,n_jobs=-1,random_state=seed, class_weight='balanced')\n",
    "        rf_X, rf_y = RandomUnderSampler(sampling_strategy=0.5, random_state=seed).fit_resample(rf_train_X, rf_train_y)\n",
    "        rf_X, rf_y = SMOTE(k_neighbors=100, random_state=seed).fit_resample(rf_X, rf_y)\n",
    "        # rf_X, rf_y = SMOTE(k_neighbors=100, random_state=seed).fit_resample(rf_train_X, rf_train_y)\n",
    "        rf.fit(rf_X,rf_y)\n",
    "        rf_ouput = open(train_rf_model_path, 'wb')\n",
    "        pickle.dump(rf,rf_ouput)\n",
    "        print(\"finish to creat a new model\")\n",
    "    else:\n",
    "        with open(train_rf_model_path,'rb') as f:\n",
    "            rf = pickle.load(f)\n",
    "    printResult(rf_test_X,rf_test_y,rf)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e1f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a dictionary include the correctly predicted files by our rf model.\n",
    "def getCorrectedPredictFileDict(model,data,test_X,test_y,divider):\n",
    "    predict_counter = 0;\n",
    "    correct_prediction_dict = {}\n",
    "    predic_set = []\n",
    "    test_dataset_row = data[divider:]\n",
    "    predict_label = model.predict(test_X)\n",
    "    for index in range(len(predict_label)):\n",
    "        if(predict_label[index] == test_y[index] and test_y[index] == 1):\n",
    "            predict_counter += 1\n",
    "            changeId = test_dataset_row[index][2]\n",
    "            fileName = test_dataset_row[index][1]\n",
    "            if changeId in correct_prediction_dict:\n",
    "                correct_prediction_dict[changeId][fileName] = test_X[index]\n",
    "            else:\n",
    "                correct_prediction_dict[changeId] = {fileName:test_X[index]}\n",
    "            predic_set.append((changeId,fileName))\n",
    "    return correct_prediction_dict,set(predic_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b56061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a dictionary include all lines in the correctly predicted files.\n",
    "def getCorrectedPredictLineDict(correct_prediction_dict,line_data, ngram=False):\n",
    "    line_check_counter = 0\n",
    "    line_predict_dict = {}\n",
    "    line_predict_set = []\n",
    "    for line_element in line_data:\n",
    "        line_predict_fileName = line_element[2]\n",
    "        line_predict_changeId = line_element[3]\n",
    "        if ngram == True:\n",
    "            line_predict_code = str(line_element[6]).replace(\"<NUMBER>\",\"\")\n",
    "        else:\n",
    "            line_predict_code = line_element[6]\n",
    "        line_predict_loc = line_element[9]\n",
    "        line_predict_label = line_element[0]\n",
    "        if line_predict_changeId in correct_prediction_dict:\n",
    "            if line_predict_fileName in correct_prediction_dict[line_predict_changeId]:\n",
    "                if line_predict_changeId in line_predict_dict:\n",
    "                    if line_predict_fileName in line_predict_dict[line_predict_changeId]:\n",
    "                        line_predict_dict[line_predict_changeId][line_predict_fileName].append((line_predict_code, line_predict_loc, line_predict_label))\n",
    "                    else:\n",
    "                        line_predict_dict[line_predict_changeId][line_predict_fileName] = [(line_predict_code, line_predict_loc, line_predict_label)]\n",
    "                else:\n",
    "                    line_predict_dict[line_predict_changeId] = {line_predict_fileName:[(line_predict_code, line_predict_loc, line_predict_label)]}\n",
    "                line_predict_set.append((line_predict_changeId,line_predict_fileName))\n",
    "    return line_predict_dict,set(line_predict_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0063cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistributionLineInTrain(row_data, line_row_data,divider):\n",
    "    train_row_data_contents = row_data[:divider]\n",
    "    train_row_data_dict = {}\n",
    "    #Build a dict only includes the training data.\n",
    "    for train_row_data_content in train_row_data_contents:\n",
    "        train_row_data_label = train_row_data_content[0]\n",
    "        train_row_data_file = train_row_data_content[1]\n",
    "        train_row_data_changeId = train_row_data_content[2]\n",
    "        if train_row_data_label == 1:\n",
    "            if train_row_data_changeId in train_row_data_dict:\n",
    "                train_row_data_dict[train_row_data_changeId].append(train_row_data_file)\n",
    "            else:\n",
    "                train_row_data_dict[train_row_data_changeId] = [train_row_data_file]\n",
    "    line_row_data_dict = {}\n",
    "    code_length_distribution = []\n",
    "    for line_code_content in line_row_data:\n",
    "        train_row_line_data_label = line_code_content[0]\n",
    "        train_row_line_data_file = line_code_content[2]\n",
    "        train_row_line_data_changeId = line_code_content[3]\n",
    "        train_row_line_data_code = line_code_content[6]\n",
    "        #check if the LOC's file and change id in the training dataset \n",
    "        if train_row_line_data_changeId in train_row_data_dict and train_row_line_data_file in train_row_data_dict[train_row_line_data_changeId] and train_row_line_data_label == 1:\n",
    "            #Save to a new dict\n",
    "            code_length_distribution.append(len(str(train_row_line_data_code).split()))\n",
    "            if train_row_line_data_changeId in line_row_data_dict:\n",
    "                if train_row_line_data_file in line_row_data_dict[train_row_line_data_changeId]:\n",
    "                    line_row_data_dict[train_row_line_data_changeId][train_row_line_data_file].append(train_row_line_data_code)\n",
    "                else:\n",
    "                    line_row_data_dict[train_row_line_data_changeId] = {train_row_line_data_file:[train_row_line_data_code]}\n",
    "            else:\n",
    "                line_row_data_dict[train_row_line_data_changeId] = {train_row_line_data_file:[train_row_line_data_code]}\n",
    "    length_count_dict = {}\n",
    "    (unique, counts) = np.unique(code_length_distribution,return_counts=True)\n",
    "    for index in range(len(unique)):\n",
    "        length_count_dict[unique[index]] = float(counts[index]/len(code_length_distribution))\n",
    "    return length_count_dict,code_length_distribution,line_row_data_dict,train_row_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6323bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the LIME score \n",
    "def getLimeExplainerScoreDict(project,correct_prediction_dict,data_count_vect,lime_train_X,model,seed,bias):\n",
    "    explainer_feature_path = explainer_model_path + '/feature_ouput_' + project +'-'+ str(seed)+'-'+str(bias)+'.pkl' \n",
    "\n",
    "    if not os.path.exists(explainer_feature_path): \n",
    "        feature_dict = {}\n",
    "                \n",
    "        stop_tokens = [\"*deletedLine\",\"*addedLine\",'*changedLine']\n",
    "        python_common_tokens = []\n",
    "        stop_tokens = [\"*deletedLine\",\"*addedLine\",\"*changedLine\",\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "        python_common_tokens = ['abs','delattr','hash','memoryview','set','all','dict','help','min','setattr','any','dir','hex','next','slice','ascii','divmod','id','object','sorted','bin','enumerate','input','oct','staticmethod','bool','eval','int','open','str','breakpoint','exec','isinstance','ord','sum','bytearray','filter','issubclass','pow','super','bytes','float','iter','print','tuple','callable','format','len','property','type','chr','frozenset','list','range','vars','classmethod','getattr','locals','repr','zip','compile','globals','map','reversed','import','complex','hasattr','max','round','False','await','else','import','passNone','break','except','in','raise','True','class','finally','is','return','and','continue','for','lambda','try','as','def','from','nonlocal','while','assert','del','global','not','with','async','elif','if','or','yield', 'self']\n",
    "        c_common_tokens = ['auto','const','double','float','int','short','struct','unsigned','break','continue','else','for','long','signed','switch','void','case','default','enum','goto','register','sizeof','typedef','volatile','char','do','extern','if','return','static','union','while','asm','namespace','try','bool','explicit','new','typeid','catch','false','operator','template','typename','class','friend','private','this','using','inline','public','throw','virtual','delete','mutable','protected','true']\n",
    "        if project == 'nova' or project == 'ironic':\n",
    "            print(\"using python stopword\")\n",
    "            common_tokens = python_common_tokens\n",
    "        if project == 'base':\n",
    "            print(\"using c stopword\")\n",
    "            common_tokens = c_common_tokens\n",
    "        all_features = np.concatenate([data_count_vect.get_feature_names_out(), ['*deletedLine','*addedLine','*changedLine']])\n",
    "        explainer = LimeTabularExplainer(lime_train_X, \n",
    "                                          feature_names=all_features, \n",
    "                                          class_names=['True','False'],\n",
    "                                          discretize_continuous=False, random_state=seed\n",
    "                                         )\n",
    "        feature_training_counter = 0\n",
    "        print(\"Training explainer\")\n",
    "        for key,value in correct_prediction_dict.items():\n",
    "            for fileName, codeArray in value.items():\n",
    "                print(feature_training_counter)\n",
    "                exp = explainer.explain_instance(codeArray,model.predict_proba,num_features=len(all_features), top_labels=1)\n",
    "                features_val = exp.as_list(label=1)\n",
    "                new_features_val = [tup for tup in features_val]\n",
    "                if key in feature_dict:\n",
    "                    feature_dict[key][fileName] = {val[0]:val[1] for val in new_features_val if val[0] not in stop_tokens and val[0] not in common_tokens}\n",
    "                else:\n",
    "                    feature_dict[key] = {fileName: {val[0]:val[1] for val in new_features_val if val[0] not in stop_tokens and val[0] not in common_tokens}}\n",
    "                feature_training_counter += 1\n",
    "                \n",
    "        feature_ouput = open(explainer_feature_path, 'wb')\n",
    "        pickle.dump(feature_dict,feature_ouput)\n",
    "        \n",
    "        print(\"write feature output to pickle\")\n",
    "    else:\n",
    "        with open(explainer_feature_path, \"rb\") as f:\n",
    "            feature_dict = pickle.load(f)\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e1ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ngram\n",
    "# 1. Run functions\"outputSlpTestFile\",\"outputSlpTrainFile\",\"generateNgramFiles\" \n",
    "# 2. A training file \"ngram_train_PORJECTNAME_2.csv\" and a testing file \"ngram_test_PORJECTNAME_2.csv\" for ngram approach will be geenrated in the path \"data_process/commented/dataset/eval_file/\"\n",
    "# 2. Put these two files to the path \"/SLP-Core/src/main/java/slp/core/example/\" and follow the code comments in EntrpoyForEachLine.java. It would generate result file for ngram named \"entropy_PROJECT_2.csv\" \n",
    "# 3. Then put result of ngram file \"entropy_PROJECT_2.csv\" to the path \"data_process/commented/dataset/eval_file/\"\n",
    "# 4. Run rest of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfbb56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate test dataset for ngram baseline approach\n",
    "#Output files \"ngram_test_PORJECTNAME_2.csv\"\n",
    "def outputSlpTestFile(project,line_predict_dict_ngram,seed):\n",
    "    csv_ngram_path = eval_file_path + '/ngram_test_'+ project +'-'+str(seed)+'.csv'\n",
    "    csv_lime_path = eval_file_path + '/lime_test_'+ project +'-'+str(seed)+'.csv'\n",
    "    csv_ngram_file = open(csv_ngram_path,\"w\")\n",
    "    csv_lime_file = open(csv_lime_path,\"w\")\n",
    "    csv_ngram_writer = csv.writer(csv_ngram_file,quoting=csv.QUOTE_NONE,escapechar='　') \n",
    "    csv_lime_writer = csv.writer(csv_lime_file) \n",
    "    for key,value in line_predict_dict_ngram.items():\n",
    "        for fileName, codeLines in value.items():\n",
    "            for codeLine in codeLines:\n",
    "                csv_ngram_writer.writerow([codeLine[0]])\n",
    "                csv_lime_writer.writerow([key, fileName, codeLine[0].strip(),codeLine[1],codeLine[2]])\n",
    "    csv_ngram_file.close()  \n",
    "    csv_lime_file.close()\n",
    "    print(\"write test datset done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e1bbf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate training dataset for ngram baseline approach\n",
    "#Output files \"ngram_train_PORJECTNAME_2.csv\"\n",
    "def outputSlpTrainFile(project,seed):\n",
    "    capitalied_project = project.capitalize()\n",
    "    data_ngram_train = pd.read_csv(eval_file_path + '/ngramTrain' + capitalied_project + '.csv', dtype=None, sep=',',header=None).to_numpy().tolist()\n",
    "    data_train_ngram_test = pd.read_csv(eval_file_path + '/ngram_test_' + project + '-'+str(seed)+'.csv', dtype=None, sep=',',header=None).to_numpy().tolist()\n",
    "    data_ngram_train = [x[0] for x in data_ngram_train]\n",
    "    data_train_ngram_test = [x[0].strip() for x in data_train_ngram_test]\n",
    "    new_data_train_ngram_test = list(set(data_ngram_train) - set(data_train_ngram_test))\n",
    "\n",
    "    csv_ngram_train_path = eval_file_path + '/ngram_train_'+ project +'-'+str(seed)+'.csv'\n",
    "    csv_new_train_ngram_file = open(csv_ngram_train_path,\"w\")\n",
    "    csv_ngram_train_writer = csv.writer(csv_new_train_ngram_file,quoting=csv.QUOTE_NONE,escapechar='　') \n",
    "    for code in new_data_train_ngram_test:\n",
    "        csv_ngram_train_writer.writerow([code])\n",
    "    csv_new_train_ngram_file.close()  \n",
    "    print(\"write train datset done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b25358d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output ngram training/test dataset\n",
    "def generateNgramFiles(seed,projectName,data,line_data,bias):\n",
    "    projectName = projectName\n",
    "    train_X, train_y, test_X, test_y,divider,data_count_vect = getDatasetFromRawData(projectName,data, bias)\n",
    "    rf = trainRFmodel(projectName,train_X, train_y, test_X, test_y,seed,bias)\n",
    "    correct_prediction_dict, predic_set = getCorrectedPredictFileDict(rf,data,test_X,test_y,divider)\n",
    "    line_predict_dict_ngram,line_predict_set_ngram = getCorrectedPredictLineDict(correct_prediction_dict,line_data, ngram=True)\n",
    "    outputSlpTestFile(projectName,line_predict_dict_ngram,seed)\n",
    "    outputSlpTrainFile(projectName,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7beb970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate ngram rank score\n",
    "def evalLineNgram(project,seed):\n",
    "    data_entropy = pd.read_csv(eval_file_path + '/entropy_'+project+'-'+str(seed)+'.csv', dtype=None, sep=',',header=None).to_numpy()\n",
    "    data_ngram_test = pd.read_csv(eval_file_path + '/lime_test_'+project+'-'+str(seed)+'.csv', dtype=None, sep=',',header=None).to_numpy()\n",
    "\n",
    "    ngram_eval_line = {}\n",
    "    for index in range(len(data_ngram_test)):\n",
    "        eval_line_ngram_changeId = data_ngram_test[index][0]\n",
    "        eval_line_ngram_fileName = data_ngram_test[index][1]\n",
    "        eval_line_ngram_code = data_ngram_test[index][2]\n",
    "        eval_line_ngram_line = data_ngram_test[index][3]\n",
    "        eval_line_ngram_label = data_ngram_test[index][4]\n",
    "        eval_line_entropy = data_entropy[index][0]\n",
    "        if eval_line_ngram_changeId in ngram_eval_line:\n",
    "            if eval_line_ngram_fileName in ngram_eval_line[eval_line_ngram_changeId]:\n",
    "                ngram_eval_line[eval_line_ngram_changeId][eval_line_ngram_fileName].append((eval_line_ngram_code,eval_line_ngram_line,eval_line_ngram_label,eval_line_entropy))\n",
    "            else:\n",
    "                ngram_eval_line[eval_line_ngram_changeId][eval_line_ngram_fileName] = [(eval_line_ngram_code,eval_line_ngram_line,eval_line_ngram_label,eval_line_entropy)]\n",
    "        else:\n",
    "            ngram_eval_line[eval_line_ngram_changeId] = {eval_line_ngram_fileName:[(eval_line_ngram_code,eval_line_ngram_line,eval_line_ngram_label,eval_line_entropy)]}\n",
    "    return ngram_eval_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48a3d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate csv files fr our approach\n",
    "def generateRawDataCsv(result,project,eval_dict_ngram,length_count_dict):\n",
    "    for changeId, pair in eval_dict_ngram.items():\n",
    "        for fileName, value in pair.items():\n",
    "            for index, element in enumerate(value):\n",
    "                token_length = len(str(element[0]).split())\n",
    "                length_score = length_count_dict.get(token_length,0)\n",
    "                result.append({\"dataset\": project, \"changeId\":changeId, \"fileName\": fileName, \"lineNumber\":element[1], \"token\":element[0], \"ngramScore\":str(element[3]), \"groundTruth\":element[2], \"lengthScore\":length_score})\n",
    "    return result\n",
    "\n",
    "def generateLimeScoreDataCsv(result,project,feature_dict,line_predict_dict):\n",
    "    for changeId, pair in line_predict_dict.items():\n",
    "        for fileName, value in pair.items():\n",
    "            for index, element in enumerate(value):\n",
    "                code = list(set(element[0].lower().split()))\n",
    "                for token in code:\n",
    "                    token_score = feature_dict[changeId][fileName].get(token,0)\n",
    "                    result.append({\"dataset\": project, \"changeId\":changeId, \"fileName\": fileName, \"lineNumber\":element[1],\"token\":token,\"limeScore\":token_score})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6b3ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate result for our approach\n",
    "def result(seed,projectName,data,line_data,bias):\n",
    "    projectName = projectName\n",
    "    #RF\n",
    "    train_X, train_y, test_X, test_y,divider,data_count_vect = getDatasetFromRawData(projectName,data, bias)\n",
    "    rf = trainRFmodel(projectName,train_X, train_y, test_X, test_y,seed,bias)\n",
    "    correct_prediction_dict, predic_set = getCorrectedPredictFileDict(rf,data,test_X,test_y,divider)\n",
    "    line_predict_dict,line_predict_set = getCorrectedPredictLineDict(correct_prediction_dict,line_data)\n",
    "    feature_dict = getLimeExplainerScoreDict(projectName,correct_prediction_dict,data_count_vect,train_X,rf,seed,bias)\n",
    "    length_count_dict,code_length_distribution,line_row_data_dict,train_row_data_dict = getDistributionLineInTrain(data,line_data,divider)\n",
    "    eval_dict_ngram = evalLineNgram(projectName,seed)\n",
    "\n",
    "    raw_data_result = []\n",
    "    lime_score_result = []   \n",
    "    raw_data_result = generateRawDataCsv(raw_data_result,projectName,eval_dict_ngram,length_count_dict)\n",
    "    lime_score_result = generateLimeScoreDataCsv(lime_score_result,projectName,feature_dict,line_predict_dict)\n",
    "    return raw_data_result,lime_score_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ae9995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output csv files at path: ./dataset/csv/\n",
    "def outputcsv():\n",
    "    raw_data_result_nova,lime_score_result_nova = result(2,\"nova\",data_nova,line_data_nova,5) \n",
    "    #raw_data_result_ironic,lime_score_result_ironic = result(2,\"ironic\",data_ironic,line_data_ironic,-4) \n",
    "    raw_data_result_base,lime_score_result_base = result(2,\"base\",data_base,line_data_base,6)\n",
    "    raw_result = raw_data_result_nova + raw_data_result_base\n",
    "    lime_result = lime_score_result_nova + lime_score_result_base\n",
    "\n",
    "    csv_path_raw = './dataset/csv/csv_commented_lineLevel_raw.csv'\n",
    "    csv_file_raw = open(csv_path_raw,\"w\")\n",
    "    fieldnames_raw = ['dataset','changeId','fileName','lineNumber','token','ngramScore','groundTruth','lengthScore']\n",
    "    print(\"generating csv, length:\", len(raw_result))\n",
    "    csv_writer_raw = csv.DictWriter(csv_file_raw,quoting=csv.QUOTE_NONE,escapechar=';', fieldnames= fieldnames_raw) \n",
    "    csv_writer_raw.writeheader()\n",
    "    for row in raw_result:\n",
    "        csv_writer_raw.writerow(row) \n",
    "    print(\"csv generated\")\n",
    "\n",
    "    csv_path_raw = './dataset/csv/csv_commented_limescore.csv'\n",
    "    csv_file_raw = open(csv_path_raw,\"w\")\n",
    "    fieldnames_raw = ['dataset','changeId','fileName','lineNumber','token','limeScore']\n",
    "    print(\"generating csv, length:\", len(raw_result))\n",
    "    csv_writer_raw = csv.DictWriter(csv_file_raw,quoting=csv.QUOTE_NONE,escapechar=';', fieldnames= fieldnames_raw) \n",
    "    csv_writer_raw.writeheader()\n",
    "    for row in lime_result:\n",
    "        csv_writer_raw.writerow(row) \n",
    "    print(\"csv generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37066e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "AUC: 0.8396746203904555\n",
      "Precision: 0.17938931297709923\n",
      "Recall: 0.5222222222222223\n",
      "F1: 0.26704545454545453\n",
      "Confusion matrix: \n",
      " [[2090  215]\n",
      " [  43   47]]\n",
      "write test datset done\n",
      "write train datset done\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "AUC: 0.7408735687889986\n",
      "Precision: 0.11736334405144695\n",
      "Recall: 0.4294117647058823\n",
      "F1: 0.18434343434343436\n",
      "Confusion matrix: \n",
      " [[3335  549]\n",
      " [  97   73]]\n",
      "write test datset done\n",
      "write train datset done\n"
     ]
    }
   ],
   "source": [
    "#Generate ngram training/test dataset for studied projects\n",
    "generateNgramFiles(2,\"nova\",data_nova,line_data_nova,5)\n",
    "# generateNgramFiles(2,\"ironic\",data_ironic,line_data_ironic,-4)\n",
    "generateNgramFiles(2,\"base\",data_base,line_data_base,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cab2a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "AUC: 0.8396746203904555\n",
      "Precision: 0.17938931297709923\n",
      "Recall: 0.5222222222222223\n",
      "F1: 0.26704545454545453\n",
      "Confusion matrix: \n",
      " [[2090  215]\n",
      " [  43   47]]\n",
      "using python stopword\n",
      "Training explainer\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "write feature output to pickle\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "AUC: 0.7408735687889986\n",
      "Precision: 0.11736334405144695\n",
      "Recall: 0.4294117647058823\n",
      "F1: 0.18434343434343436\n",
      "Confusion matrix: \n",
      " [[3335  549]\n",
      " [  97   73]]\n",
      "using c stopword\n",
      "Training explainer\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "write feature output to pickle\n",
      "generating csv, length: 7376\n",
      "csv generated\n",
      "generating csv, length: 7376\n",
      "csv generated\n"
     ]
    }
   ],
   "source": [
    "#Generate csv files for evaluation and data evaluation in R scripts\n",
    "outputcsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06155465-569b-4f75-8894-d10b224f00be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a6179-e406-421d-9ec1-76c1eb4447b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07c497-384b-4cd2-9781-ca6793323497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccdfb47-dab8-4f6c-9429-bec100ff1445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
