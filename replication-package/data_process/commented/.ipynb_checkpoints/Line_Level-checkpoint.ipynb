{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b8bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scripts information:\n",
    "#Input: File level dataset. Files: \"Nova.csv\",\"Ironic.csv\", \"Base.csv\"\n",
    "#Output: File level result. File: \"csv_commented_lineLevel_raw.csv\" includes 'dataset','changeId','fileName','lineNumber','token','ngramScore','groundTruth','lengthScore'\n",
    "#                                  \"csv_commented_limescore.csv\" includes 'dataset','changeId','fileName','lineNumber','token','limeScore'\n",
    "#Description: This script is used to generate the csv files for predicting lines to be commented for RQ2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from scipy import sparse\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import time, pickle, math, warnings, os, operator\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# path that saves prediction result from n-gram model \n",
    "eval_file_path = './dataset/eval_file'\n",
    "\n",
    "# path that saves all trained models\n",
    "model_path = './dataset/ml-model'\n",
    "\n",
    "# path that saves all LIME models\n",
    "explainer_model_path = './dataset/lime-feature-model'\n",
    "\n",
    "#load file level dataset\n",
    "data_nova = pd.read_csv('./dataset/fileLevel/OpenStack_ex.csv', dtype=None, sep=',').to_numpy()\n",
    "# data_ironic = pd.read_csv('./dataset/fileLevel/Ironic.csv', dtype=None, sep=',').to_numpy()\n",
    "data_base = pd.read_csv('./dataset/fileLevel/Base_ex.csv', dtype=None, sep=',').to_numpy()\n",
    "#load line level dataset\n",
    "line_data_nova = pd.read_csv('./dataset/lineLevel/dataset-openstack-linelevel/OpenStack_ex.csv', dtype=None, sep=',').to_numpy()\n",
    "# line_data_ironic = pd.read_csv('./dataset/lineLevel/dataset-openstack-linelevel/Ironic.csv', dtype=None, sep=',').to_numpy()\n",
    "line_data_base = pd.read_csv('./dataset/lineLevel/dataset-qt-linelevel/qt_base_ex.csv', dtype=None, sep=',').to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b30675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate training/test datset\n",
    "def getDatasetFromRawData(project_source,data, bias):\n",
    "    row_data = data[0:,3]\n",
    "    row_data_Y = data[0:,0]\n",
    "    if project_source == \"base\":\n",
    "        row_data_deletions = data[0:,6]\n",
    "        row_data_additions = data[0:,7]\n",
    "        row_data_changedLine = data[0:,8]\n",
    "    else:\n",
    "        row_data_deletions = data[0:,7]\n",
    "        row_data_additions = data[0:,8]\n",
    "        row_data_changedLine = data[0:,9]\n",
    "    Y_train = []\n",
    "    is_comment = 0\n",
    "    not_comment = 0\n",
    "    for element in row_data_Y:\n",
    "        if(element == 0):\n",
    "            Y_train.append(False)\n",
    "            not_comment += 1\n",
    "        else:\n",
    "            Y_train.append(True)\n",
    "            is_comment += 1\n",
    "    Y_train = np.array(Y_train)\n",
    "\n",
    "    #finding a index that wouldn't separate file in same changeId into both training dataset and test dataset\n",
    "    first = int(len(data)*0.6)\n",
    "    divider = int(len(data)*0.2) + first + bias\n",
    "    data_count_vect = CountVectorizer(min_df=2, max_df=0.5)\n",
    "    train_row_data = row_data[:divider]\n",
    "    test_row_data = row_data[divider:]\n",
    "    train_row_data_deletions = row_data_deletions[:divider]\n",
    "    test_row_data_deletions = row_data_deletions[divider:]\n",
    "    train_row_data_additions = row_data_additions[:divider]\n",
    "    test_row_data_additions = row_data_additions[divider:]\n",
    "    train_row_data_changedLine = row_data_changedLine[:divider]\n",
    "    test_row_data_changedLine = row_data_changedLine[divider:]\n",
    "    data_train_counts = data_count_vect.fit_transform(train_row_data)\n",
    "    data_test_counts = data_count_vect.transform(test_row_data)\n",
    "    final_train_X = np.hstack((data_train_counts.toarray(),train_row_data_deletions[:,None]))\n",
    "    final_train_X = np.hstack((final_train_X,train_row_data_additions[:,None]))\n",
    "    final_train_X = np.hstack((final_train_X,train_row_data_changedLine[:,None]))\n",
    "    final_test_X = np.hstack((data_test_counts.toarray(),test_row_data_deletions[:,None]))\n",
    "    final_test_X = np.hstack((final_test_X,test_row_data_additions[:,None]))\n",
    "    final_test_X = np.hstack((final_test_X,test_row_data_changedLine[:,None]))\n",
    "    final_train_y = Y_train[:divider]\n",
    "    final_test_y = Y_train[divider:]\n",
    "    print(type(data_count_vect))\n",
    "    data_count_vect.fit_transform(train_row_data)\n",
    "    print(type(data_count_vect))\n",
    "    del data_train_counts,data_test_counts,train_row_data,test_row_data,data,row_data,row_data_Y\n",
    "    return final_train_X,final_train_y,final_test_X,final_test_y,divider,data_count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f784d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResult(x, y, model):\n",
    "    print(\"AUC:\",roc_auc_score(y, model.predict_proba(x)[:,1]))\n",
    "    print(\"Precision:\",precision_score(y, model.predict(x)))\n",
    "    print(\"Recall:\",recall_score(y, model.predict(x)))\n",
    "    print(\"F1:\",f1_score(y, model.predict(x)))\n",
    "    print(\"Confusion matrix: \\n\",confusion_matrix(y, model.predict(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989a2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRFmodel(project,rf_train_X,rf_train_y,rf_test_X,rf_test_y,seed,bias):\n",
    "    train_rf_model_path = model_path+'/smote_abstr_number_df_2_rf_'+project+'-'+str(seed)+str(bias)+'.pkl'\n",
    "    if not os.path.exists(train_rf_model_path):\n",
    "        rf = RandomForestClassifier(n_estimators=200,n_jobs=-1,random_state=seed, class_weight='balanced')\n",
    "        rf_X, rf_y = RandomUnderSampler(sampling_strategy=0.5, random_state=seed).fit_resample(rf_train_X, rf_train_y)\n",
    "        rf_X, rf_y = SMOTE(k_neighbors=100, random_state=seed).fit_resample(rf_X, rf_y)\n",
    "        # rf_X, rf_y = SMOTE(k_neighbors=100, random_state=seed).fit_resample(rf_train_X, rf_train_y)\n",
    "        rf.fit(rf_X,rf_y)\n",
    "        rf_ouput = open(train_rf_model_path, 'wb')\n",
    "        pickle.dump(rf,rf_ouput)\n",
    "        print(\"finish to creat a new model\")\n",
    "    else:\n",
    "        with open(train_rf_model_path,'rb') as f:\n",
    "            rf = pickle.load(f)\n",
    "    printResult(rf_test_X,rf_test_y,rf)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e1f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a dictionary include the correctly predicted files by our rf model.\n",
    "def getCorrectedPredictFileDict(model,data,test_X,test_y,divider):\n",
    "    predict_counter = 0;\n",
    "    correct_prediction_dict = {}\n",
    "    predic_set = []\n",
    "    test_dataset_row = data[divider:]\n",
    "    predict_label = model.predict(test_X)\n",
    "    for index in range(len(predict_label)):\n",
    "        if(predict_label[index] == test_y[index] and test_y[index] == 1):\n",
    "            predict_counter += 1\n",
    "            changeId = test_dataset_row[index][2]\n",
    "            fileName = test_dataset_row[index][1]\n",
    "            if changeId in correct_prediction_dict:\n",
    "                correct_prediction_dict[changeId][fileName] = test_X[index]\n",
    "            else:\n",
    "                correct_prediction_dict[changeId] = {fileName:test_X[index]}\n",
    "            predic_set.append((changeId,fileName))\n",
    "    return correct_prediction_dict,set(predic_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b56061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a dictionary include all lines in the correctly predicted files.\n",
    "def getCorrectedPredictLineDict(correct_prediction_dict,line_data, ngram=False):\n",
    "    line_check_counter = 0\n",
    "    line_predict_dict = {}\n",
    "    line_predict_set = []\n",
    "    for line_element in line_data:\n",
    "        line_predict_fileName = line_element[2]\n",
    "        line_predict_changeId = line_element[3]\n",
    "        if ngram == True:\n",
    "            line_predict_code = str(line_element[6]).replace(\"<NUMBER>\",\"\")\n",
    "        else:\n",
    "            line_predict_code = line_element[6]\n",
    "        line_predict_loc = line_element[9]\n",
    "        line_predict_label = line_element[0]\n",
    "        if line_predict_changeId in correct_prediction_dict:\n",
    "            if line_predict_fileName in correct_prediction_dict[line_predict_changeId]:\n",
    "                if line_predict_changeId in line_predict_dict:\n",
    "                    if line_predict_fileName in line_predict_dict[line_predict_changeId]:\n",
    "                        line_predict_dict[line_predict_changeId][line_predict_fileName].append((line_predict_code, line_predict_loc, line_predict_label))\n",
    "                    else:\n",
    "                        line_predict_dict[line_predict_changeId][line_predict_fileName] = [(line_predict_code, line_predict_loc, line_predict_label)]\n",
    "                else:\n",
    "                    line_predict_dict[line_predict_changeId] = {line_predict_fileName:[(line_predict_code, line_predict_loc, line_predict_label)]}\n",
    "                line_predict_set.append((line_predict_changeId,line_predict_fileName))\n",
    "    return line_predict_dict,set(line_predict_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0063cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistributionLineInTrain(row_data, line_row_data,divider):\n",
    "    train_row_data_contents = row_data[:divider]\n",
    "    train_row_data_dict = {}\n",
    "    #Build a dict only includes the training data.\n",
    "    for train_row_data_content in train_row_data_contents:\n",
    "        train_row_data_label = train_row_data_content[0]\n",
    "        train_row_data_file = train_row_data_content[1]\n",
    "        train_row_data_changeId = train_row_data_content[2]\n",
    "        if train_row_data_label == 1:\n",
    "            if train_row_data_changeId in train_row_data_dict:\n",
    "                train_row_data_dict[train_row_data_changeId].append(train_row_data_file)\n",
    "            else:\n",
    "                train_row_data_dict[train_row_data_changeId] = [train_row_data_file]\n",
    "    line_row_data_dict = {}\n",
    "    code_length_distribution = []\n",
    "    for line_code_content in line_row_data:\n",
    "        train_row_line_data_label = line_code_content[0]\n",
    "        train_row_line_data_file = line_code_content[2]\n",
    "        train_row_line_data_changeId = line_code_content[3]\n",
    "        train_row_line_data_code = line_code_content[6]\n",
    "        #check if the LOC's file and change id in the training dataset \n",
    "        if train_row_line_data_changeId in train_row_data_dict and train_row_line_data_file in train_row_data_dict[train_row_line_data_changeId] and train_row_line_data_label == 1:\n",
    "            #Save to a new dict\n",
    "            code_length_distribution.append(len(str(train_row_line_data_code).split()))\n",
    "            if train_row_line_data_changeId in line_row_data_dict:\n",
    "                if train_row_line_data_file in line_row_data_dict[train_row_line_data_changeId]:\n",
    "                    line_row_data_dict[train_row_line_data_changeId][train_row_line_data_file].append(train_row_line_data_code)\n",
    "                else:\n",
    "                    line_row_data_dict[train_row_line_data_changeId] = {train_row_line_data_file:[train_row_line_data_code]}\n",
    "            else:\n",
    "                line_row_data_dict[train_row_line_data_changeId] = {train_row_line_data_file:[train_row_line_data_code]}\n",
    "    length_count_dict = {}\n",
    "    (unique, counts) = np.unique(code_length_distribution,return_counts=True)\n",
    "    for index in range(len(unique)):\n",
    "        length_count_dict[unique[index]] = float(counts[index]/len(code_length_distribution))\n",
    "    return length_count_dict,code_length_distribution,line_row_data_dict,train_row_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6323bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the LIME score \n",
    "def getLimeExplainerScoreDict(project,correct_prediction_dict,data_count_vect,lime_train_X,model,seed,bias):\n",
    "    explainer_feature_path = explainer_model_path + '/feature_ouput_' + project +'-'+ str(seed)+'-'+str(bias)+'.pkl' \n",
    "\n",
    "    if not os.path.exists(explainer_feature_path): \n",
    "        feature_dict = {}\n",
    "                \n",
    "        stop_tokens = [\"*deletedLine\",\"*addedLine\",'*changedLine']\n",
    "        python_common_tokens = []\n",
    "        stop_tokens = [\"*deletedLine\",\"*addedLine\",\"*changedLine\",\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "        python_common_tokens = ['abs','delattr','hash','memoryview','set','all','dict','help','min','setattr','any','dir','hex','next','slice','ascii','divmod','id','object','sorted','bin','enumerate','input','oct','staticmethod','bool','eval','int','open','str','breakpoint','exec','isinstance','ord','sum','bytearray','filter','issubclass','pow','super','bytes','float','iter','print','tuple','callable','format','len','property','type','chr','frozenset','list','range','vars','classmethod','getattr','locals','repr','zip','compile','globals','map','reversed','import','complex','hasattr','max','round','False','await','else','import','passNone','break','except','in','raise','True','class','finally','is','return','and','continue','for','lambda','try','as','def','from','nonlocal','while','assert','del','global','not','with','async','elif','if','or','yield', 'self']\n",
    "        c_common_tokens = ['auto','const','double','float','int','short','struct','unsigned','break','continue','else','for','long','signed','switch','void','case','default','enum','goto','register','sizeof','typedef','volatile','char','do','extern','if','return','static','union','while','asm','namespace','try','bool','explicit','new','typeid','catch','false','operator','template','typename','class','friend','private','this','using','inline','public','throw','virtual','delete','mutable','protected','true']\n",
    "        if project == 'nova' or project == 'ironic':\n",
    "            print(\"using python stopword\")\n",
    "            common_tokens = python_common_tokens\n",
    "        if project == 'base':\n",
    "            print(\"using c stopword\")\n",
    "            common_tokens = c_common_tokens\n",
    "        all_features = data_count_vect.get_feature_names_out()\n",
    "        explainer = LimeTabularExplainer(lime_train_X, \n",
    "                                          feature_names=all_features, \n",
    "                                          class_names=['True','False'],\n",
    "                                          discretize_continuous=False, random_state=seed\n",
    "                                         )\n",
    "        feature_training_counter = 0\n",
    "        print(\"Training explainer\")\n",
    "        print(correct_prediction_dict)\n",
    "        for key,value in correct_prediction_dict.items():\n",
    "            for fileName, codeArray in value.items():\n",
    "                print(feature_training_counter)\n",
    "                exp = explainer.explain_instance(codeArray,model.predict_proba,num_features=len(all_features), top_labels=1)\n",
    "                print('exp')\n",
    "                print(len(exp))\n",
    "                features_val = exp.as_list(label=1)\n",
    "                new_features_val = [tup for tup in features_val]\n",
    "                if key in feature_dict:\n",
    "                    feature_dict[key][fileName] = {val[0]:val[1] for val in new_features_val if val[0] not in stop_tokens and val[0] not in common_tokens}\n",
    "                else:\n",
    "                    feature_dict[key] = {fileName: {val[0]:val[1] for val in new_features_val if val[0] not in stop_tokens and val[0] not in common_tokens}}\n",
    "                feature_training_counter += 1\n",
    "                \n",
    "        feature_ouput = open(explainer_feature_path, 'wb')\n",
    "        pickle.dump(feature_dict,feature_ouput)\n",
    "        \n",
    "        print(\"write feature output to pickle\")\n",
    "    else:\n",
    "        with open(explainer_feature_path, \"rb\") as f:\n",
    "            feature_dict = pickle.load(f)\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e1ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ngram\n",
    "# 1. Run functions\"outputSlpTestFile\",\"outputSlpTrainFile\",\"generateNgramFiles\" \n",
    "# 2. A training file \"ngram_train_PORJECTNAME_2.csv\" and a testing file \"ngram_test_PORJECTNAME_2.csv\" for ngram approach will be geenrated in the path \"data_process/commented/dataset/eval_file/\"\n",
    "# 2. Put these two files to the path \"/SLP-Core/src/main/java/slp/core/example/\" and follow the code comments in EntrpoyForEachLine.java. It would generate result file for ngram named \"entropy_PROJECT_2.csv\" \n",
    "# 3. Then put result of ngram file \"entropy_PROJECT_2.csv\" to the path \"data_process/commented/dataset/eval_file/\"\n",
    "# 4. Run rest of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfbb56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate test dataset for ngram baseline approach\n",
    "#Output files \"ngram_test_PORJECTNAME_2.csv\"\n",
    "def outputSlpTestFile(project,line_predict_dict_ngram,seed):\n",
    "    csv_ngram_path = eval_file_path + '/ngram_test_'+ project +'-'+str(seed)+'.csv'\n",
    "    csv_lime_path = eval_file_path + '/lime_test_'+ project +'-'+str(seed)+'.csv'\n",
    "    csv_ngram_file = open(csv_ngram_path,\"w\")\n",
    "    csv_lime_file = open(csv_lime_path,\"w\")\n",
    "    csv_ngram_writer = csv.writer(csv_ngram_file,quoting=csv.QUOTE_NONE,escapechar='　') \n",
    "    csv_lime_writer = csv.writer(csv_lime_file) \n",
    "    for key,value in line_predict_dict_ngram.items():\n",
    "        for fileName, codeLines in value.items():\n",
    "            for codeLine in codeLines:\n",
    "                csv_ngram_writer.writerow([codeLine[0]])\n",
    "                csv_lime_writer.writerow([key, fileName, codeLine[0].strip(),codeLine[1],codeLine[2]])\n",
    "    csv_ngram_file.close()  \n",
    "    csv_lime_file.close()\n",
    "    print(\"write test datset done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e1bbf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate training dataset for ngram baseline approach\n",
    "#Output files \"ngram_train_PORJECTNAME_2.csv\"\n",
    "def outputSlpTrainFile(project,seed):\n",
    "    capitalied_project = project.capitalize()\n",
    "    data_ngram_train = pd.read_csv(eval_file_path + '/ngramTrain' + capitalied_project + '.csv', dtype=None, sep=',',header=None).to_numpy().tolist()\n",
    "    data_train_ngram_test = pd.read_csv(eval_file_path + '/ngram_test_' + project + '-'+str(seed)+'.csv', dtype=None, sep=',',header=None).to_numpy().tolist()\n",
    "    data_ngram_train = [x[0] for x in data_ngram_train]\n",
    "    data_train_ngram_test = [x[0].strip() for x in data_train_ngram_test]\n",
    "    new_data_train_ngram_test = list(set(data_ngram_train) - set(data_train_ngram_test))\n",
    "\n",
    "    csv_ngram_train_path = eval_file_path + '/ngram_train_'+ project +'-'+str(seed)+'.csv'\n",
    "    csv_new_train_ngram_file = open(csv_ngram_train_path,\"w\")\n",
    "    csv_ngram_train_writer = csv.writer(csv_new_train_ngram_file,quoting=csv.QUOTE_NONE,escapechar='　') \n",
    "    for code in new_data_train_ngram_test:\n",
    "        csv_ngram_train_writer.writerow([code])\n",
    "    csv_new_train_ngram_file.close()  \n",
    "    print(\"write train datset done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b25358d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output ngram training/test dataset\n",
    "def generateNgramFiles(seed,projectName,data,line_data,bias):\n",
    "    projectName = projectName\n",
    "    train_X, train_y, test_X, test_y,divider,data_count_vect = getDatasetFromRawData(projectName,data, bias)\n",
    "    rf = trainRFmodel(projectName,train_X, train_y, test_X, test_y,seed,bias)\n",
    "    correct_prediction_dict, predic_set = getCorrectedPredictFileDict(rf,data,test_X,test_y,divider)\n",
    "    line_predict_dict_ngram,line_predict_set_ngram = getCorrectedPredictLineDict(correct_prediction_dict,line_data, ngram=True)\n",
    "    outputSlpTestFile(projectName,line_predict_dict_ngram,seed)\n",
    "    outputSlpTrainFile(projectName,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7beb970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate ngram rank score\n",
    "def evalLineNgram(project,seed):\n",
    "    data_entropy = pd.read_csv(eval_file_path + '/entropy_'+project+'-'+str(seed)+'.csv', dtype=None, sep=',',header=None).to_numpy()\n",
    "    data_ngram_test = pd.read_csv(eval_file_path + '/lime_test_'+project+'-'+str(seed)+'.csv', dtype=None, sep=',',header=None).to_numpy()\n",
    "\n",
    "    ngram_eval_line = {}\n",
    "    for index in range(len(data_ngram_test)):\n",
    "        eval_line_ngram_changeId = data_ngram_test[index][0]\n",
    "        eval_line_ngram_fileName = data_ngram_test[index][1]\n",
    "        eval_line_ngram_code = data_ngram_test[index][2]\n",
    "        eval_line_ngram_line = data_ngram_test[index][3]\n",
    "        eval_line_ngram_label = data_ngram_test[index][4]\n",
    "        eval_line_entropy = data_entropy[index][0]\n",
    "        if eval_line_ngram_changeId in ngram_eval_line:\n",
    "            if eval_line_ngram_fileName in ngram_eval_line[eval_line_ngram_changeId]:\n",
    "                ngram_eval_line[eval_line_ngram_changeId][eval_line_ngram_fileName].append((eval_line_ngram_code,eval_line_ngram_line,eval_line_ngram_label,eval_line_entropy))\n",
    "            else:\n",
    "                ngram_eval_line[eval_line_ngram_changeId][eval_line_ngram_fileName] = [(eval_line_ngram_code,eval_line_ngram_line,eval_line_ngram_label,eval_line_entropy)]\n",
    "        else:\n",
    "            ngram_eval_line[eval_line_ngram_changeId] = {eval_line_ngram_fileName:[(eval_line_ngram_code,eval_line_ngram_line,eval_line_ngram_label,eval_line_entropy)]}\n",
    "    return ngram_eval_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48a3d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate csv files fr our approach\n",
    "def generateRawDataCsv(result,project,eval_dict_ngram,length_count_dict):\n",
    "    for changeId, pair in eval_dict_ngram.items():\n",
    "        for fileName, value in pair.items():\n",
    "            for index, element in enumerate(value):\n",
    "                token_length = len(str(element[0]).split())\n",
    "                length_score = length_count_dict.get(token_length,0)\n",
    "                result.append({\"dataset\": project, \"changeId\":changeId, \"fileName\": fileName, \"lineNumber\":element[1], \"token\":element[0], \"ngramScore\":str(element[3]), \"groundTruth\":element[2], \"lengthScore\":length_score})\n",
    "    return result\n",
    "\n",
    "def generateLimeScoreDataCsv(result,project,feature_dict,line_predict_dict):\n",
    "    for changeId, pair in line_predict_dict.items():\n",
    "        for fileName, value in pair.items():\n",
    "            for index, element in enumerate(value):\n",
    "                code = list(set(element[0].lower().split()))\n",
    "                for token in code:\n",
    "                    token_score = feature_dict[changeId][fileName].get(token,0)\n",
    "                    result.append({\"dataset\": project, \"changeId\":changeId, \"fileName\": fileName, \"lineNumber\":element[1],\"token\":token,\"limeScore\":token_score})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6b3ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate result for our approach\n",
    "def result(seed,projectName,data,line_data,bias):\n",
    "    projectName = projectName\n",
    "    #RF\n",
    "    train_X, train_y, test_X, test_y,divider,data_count_vect = getDatasetFromRawData(projectName,data, bias)\n",
    "    rf = trainRFmodel(projectName,train_X, train_y, test_X, test_y,seed,bias)\n",
    "    correct_prediction_dict, predic_set = getCorrectedPredictFileDict(rf,data,test_X,test_y,divider)\n",
    "    line_predict_dict,line_predict_set = getCorrectedPredictLineDict(correct_prediction_dict,line_data)\n",
    "    feature_dict = getLimeExplainerScoreDict(projectName,correct_prediction_dict,data_count_vect,train_X,rf,seed,bias)\n",
    "    length_count_dict,code_length_distribution,line_row_data_dict,train_row_data_dict = getDistributionLineInTrain(data,line_data,divider)\n",
    "    eval_dict_ngram = evalLineNgram(projectName,seed)\n",
    "\n",
    "    raw_data_result = []\n",
    "    lime_score_result = []   \n",
    "    raw_data_result = generateRawDataCsv(raw_data_result,projectName,eval_dict_ngram,length_count_dict)\n",
    "    lime_score_result = generateLimeScoreDataCsv(lime_score_result,projectName,feature_dict,line_predict_dict)\n",
    "    return raw_data_result,lime_score_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ae9995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output csv files at path: ./dataset/csv/\n",
    "def outputcsv():\n",
    "    raw_data_result_nova,lime_score_result_nova = result(2,\"nova\",data_nova,line_data_nova,5) \n",
    "    #raw_data_result_ironic,lime_score_result_ironic = result(2,\"ironic\",data_ironic,line_data_ironic,-4) \n",
    "    raw_data_result_base,lime_score_result_base = result(2,\"base\",data_base,line_data_base,6)\n",
    "    raw_result = raw_data_result_nova + raw_data_result_base\n",
    "    lime_result = lime_score_result_nova + lime_score_result_base\n",
    "\n",
    "    csv_path_raw = './dataset/csv/csv_commented_lineLevel_raw.csv'\n",
    "    csv_file_raw = open(csv_path_raw,\"w\")\n",
    "    fieldnames_raw = ['dataset','changeId','fileName','lineNumber','token','ngramScore','groundTruth','lengthScore']\n",
    "    print(\"generating csv, length:\", len(raw_result))\n",
    "    csv_writer_raw = csv.DictWriter(csv_file_raw,quoting=csv.QUOTE_NONE,escapechar=';', fieldnames= fieldnames_raw) \n",
    "    csv_writer_raw.writeheader()\n",
    "    for row in raw_result:\n",
    "        csv_writer_raw.writerow(row) \n",
    "    print(\"csv generated\")\n",
    "\n",
    "    csv_path_raw = './dataset/csv/csv_commented_limescore.csv'\n",
    "    csv_file_raw = open(csv_path_raw,\"w\")\n",
    "    fieldnames_raw = ['dataset','changeId','fileName','lineNumber','token','limeScore']\n",
    "    print(\"generating csv, length:\", len(raw_result))\n",
    "    csv_writer_raw = csv.DictWriter(csv_file_raw,quoting=csv.QUOTE_NONE,escapechar=';', fieldnames= fieldnames_raw) \n",
    "    csv_writer_raw.writeheader()\n",
    "    for row in lime_result:\n",
    "        csv_writer_raw.writerow(row) \n",
    "    print(\"csv generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37066e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "AUC: 0.8396746203904555\n",
      "Precision: 0.17938931297709923\n",
      "Recall: 0.5222222222222223\n",
      "F1: 0.26704545454545453\n",
      "Confusion matrix: \n",
      " [[2090  215]\n",
      " [  43   47]]\n",
      "write test datset done\n",
      "write train datset done\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "AUC: 0.7408735687889986\n",
      "Precision: 0.11736334405144695\n",
      "Recall: 0.4294117647058823\n",
      "F1: 0.18434343434343436\n",
      "Confusion matrix: \n",
      " [[3335  549]\n",
      " [  97   73]]\n",
      "write test datset done\n",
      "write train datset done\n"
     ]
    }
   ],
   "source": [
    "#Generate ngram training/test dataset for studied projects\n",
    "generateNgramFiles(2,\"nova\",data_nova,line_data_nova,5)\n",
    "# generateNgramFiles(2,\"ironic\",data_ironic,line_data_ironic,-4)\n",
    "generateNgramFiles(2,\"base\",data_base,line_data_base,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cab2a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "AUC: 0.8396746203904555\n",
      "Precision: 0.17938931297709923\n",
      "Recall: 0.5222222222222223\n",
      "F1: 0.26704545454545453\n",
      "Confusion matrix: \n",
      " [[2090  215]\n",
      " [  43   47]]\n",
      "using python stopword\n",
      "Training explainer\n",
      "{'I6cd206542fdd28f3ef551dcc727f4cb35a53f6a3': {'nova/objects/instance_numa.py': array([0, 0, 0, ..., 44, 28, 72], dtype=object)}, 'I7c90961802485edda6d99caf10329b73362ef61e': {'nova/conductor/manager.py': array([0, 0, 0, ..., 30, 32, 62], dtype=object)}, 'Iff84fcba02a4b6d7717f4b387e392593f2983118': {'nova/scheduler/utils.py': array([0, 0, 0, ..., 13, 24, 37], dtype=object)}, 'Icf78fcabad8b966b6b5c289e1b660c01c928272d': {'nova/tests/unit/image/test_glance.py': array([0, 0, 0, ..., 0, 57, 57], dtype=object)}, 'I40b8caf06ed525e97e871cf381776164208461fb': {'nova/conf/compute.py': array([0, 0, 0, ..., 0, 14, 14], dtype=object)}, 'I20913201cf945a7fde1f9e6264c415e1235db7b9': {'nova/image/glance.py': array([0, 0, 0, ..., 38, 56, 94], dtype=object)}, 'I67504a37b0fe2ae5da3cba2f3122d9d0e18b9481': {'nova/compute/manager.py': array([0, 0, 0, ..., 7, 128, 135], dtype=object)}, 'I59298788f26ca8f32bf3e38f3a52f72ff63fcc8b': {'nova/compute/api.py': array([0, 0, 0, ..., 0, 41, 41], dtype=object)}, 'I03bcd6753ed776d2ee216dcdca48514e5da8c43e': {'nova/tests/functional/integrated_helpers.py': array([0, 0, 0, ..., 17, 30, 47], dtype=object)}, 'If02a263055376442a116d00c18a1599b436f21d2': {'nova/virt/libvirt/driver.py': array([0, 0, 0, ..., 39, 65, 104], dtype=object)}, 'Iee44ea525deb0b43ae43df3ba08c95ea8a4e317c': {'nova/compute/manager.py': array([0, 0, 0, ..., 22, 41, 63], dtype=object)}, 'I46846627311beb21946fd4d24048e0e3ea7ac942': {'nova/virt/libvirt/utils.py': array([0, 0, 0, ..., 35, 77, 112], dtype=object)}, 'I5d6db24a440397e588ba69f98a9cd2b8a846adc2': {'nova/tests/functional/integrated_helpers.py': array([0, 0, 0, ..., 2, 30, 32], dtype=object)}, 'Id5b15aa846a5ddaf4ac26fe586327aef8c08c89d': {'nova/virt/libvirt/driver.py': array([0, 0, 0, ..., 9, 17, 26], dtype=object)}, 'I667f56612d7f63834863476694cb1f4c71a58302': {'nova/network/neutron.py': array([0, 0, 0, ..., 0, 35, 35], dtype=object), 'nova/scheduler/request_filter.py': array([0, 0, 0, ..., 0, 65, 65], dtype=object), 'nova/tests/fixtures.py': array([1, 0, 0, ..., 8, 176, 184], dtype=object)}, 'I276d2bbce4a0390a6cfd597bb51481cc53eca918': {'nova/tests/functional/libvirt/test_numa_live_migration.py': array([0, 0, 0, ..., 2, 67, 69], dtype=object)}, 'Id4684093e8bdf3b61667490443e3d2f6ed65f4b3': {'nova/compute/manager.py': array([0, 0, 0, ..., 5, 100, 105], dtype=object), 'nova/scheduler/utils.py': array([0, 0, 0, ..., 0, 21, 21], dtype=object), 'nova/tests/functional/test_servers.py': array([0, 0, 0, ..., 11, 164, 175], dtype=object), 'nova/tests/unit/compute/test_compute.py': array([0, 0, 0, ..., 6, 381, 387], dtype=object)}, 'Id19702da05cac25192f6aa21f8f4449602ef7729': {'nova/compute/manager.py': array([0, 0, 0, ..., 45, 43, 88], dtype=object)}, 'If3e79cd71b6d0f6e535ff86b27483c137a78fee9': {'nova/compute/manager.py': array([0, 0, 0, ..., 19, 26, 45], dtype=object)}, 'I1bdc5e4971204fbfbf0dfcd232cabf2cfe02a966': {'nova/scheduler/client/report.py': array([0, 0, 0, ..., 0, 78, 78], dtype=object), 'nova/tests/unit/scheduler/client/test_report.py': array([0, 0, 0, ..., 0, 257, 257], dtype=object)}, 'I150e70629f6ae579ccfe0bf585c8a27df14fb51d': {'nova/storage/rbd_utils.py': array([0, 0, 0, ..., 20, 27, 47], dtype=object)}, 'Iabfc6fc862176120afb471c6ab0a2dbd67dfc47f': {'nova/pci/stats.py': array([0, 0, 0, ..., 24, 45, 69], dtype=object)}, 'I67455d4ecfafed96cb0f3f9fbe6f94808bd05909': {'nova/pci/stats.py': array([0, 0, 0, ..., 2, 35, 37], dtype=object)}, 'Idc035671892e4668141a93763f8f2bed7a630812': {'nova/cmd/manage.py': array([0, 0, 0, ..., 1, 128, 129], dtype=object)}, 'Idd58298a6b01775f962b9bf0a0835f762c8e0ed2': {'nova/scheduler/utils.py': array([0, 0, 0, ..., 3, 25, 28], dtype=object)}, 'I2c7b183fcb01f3cb67cb1c8b8bea7aaf5ce424f3': {'nova/tests/unit/virt/libvirt/test_host.py': array([0, 0, 0, ..., 0, 102, 102], dtype=object), 'nova/virt/libvirt/config.py': array([0, 0, 0, ..., 0, 20, 20], dtype=object), 'nova/virt/libvirt/host.py': array([0, 0, 0, ..., 0, 40, 40], dtype=object)}, 'I043880cb81b02488d13c3387d696142545c13395': {'nova/virt/libvirt/driver.py': array([0, 0, 0, ..., 14, 48, 62], dtype=object)}, 'Ie166f3b51fddeaf916cda7c5ac34bbcdda0fd17a': {'nova/network/neutron.py': array([0, 0, 0, ..., 0, 61, 61], dtype=object)}, 'I2ef7c5bef87bd64c087f3b136c2faac9a3865f10': {'nova/virt/libvirt/driver.py': array([0, 0, 0, ..., 1, 12, 13], dtype=object)}, 'I72a03e74fae8704546013d978e9c27a2573b219b': {'nova/tests/functional/libvirt/test_machine_type.py': array([0, 0, 0, ..., 0, 80, 80], dtype=object)}, 'I6b80021a2f90d3379c821dc8f02a72f350169eb3': {'nova/cmd/manage.py': array([0, 0, 0, ..., 0, 60, 60], dtype=object), 'nova/virt/machine_type_utils.py': array([0, 0, 0, ..., 0, 147, 147], dtype=object)}, 'I39909ace97f62e87f326d4d822d4e4c391445765': {'nova/virt/machine_type_utils.py': array([0, 0, 0, ..., 0, 57, 57], dtype=object)}, 'Ia5c03f011e72f34bac1fb32b3144307c4319b482': {'nova/tests/functional/libvirt/test_uefi.py': array([0, 0, 0, ..., 0, 107, 107], dtype=object)}, 'I91dd7993395f693c7d26c1caa44fa365f5cbec12': {'nova/pci/stats.py': array([0, 0, 0, ..., 3, 39, 42], dtype=object), 'nova/virt/libvirt/host.py': array([0, 0, 0, ..., 12, 138, 150], dtype=object)}, 'I2ba69d0d727cc183f4a5dc52eaf4000962caeb4a': {'nova/tests/unit/compute/test_shelve.py': array([0, 0, 0, ..., 0, 34, 34], dtype=object)}, 'If08cbe2d5f2b301ac92a183f15fb87a18836bff5': {'nova/virt/libvirt/host.py': array([0, 0, 0, ..., 0, 128, 128], dtype=object)}, 'I7f3cbc57a374b2f271018a2f6ef33ef579798db8': {'nova/compute/api.py': array([0, 0, 0, ..., 3, 76, 79], dtype=object)}}\n",
      "0\n",
      "exp\n",
      "<lime.explanation.Explanation object at 0x149ad33d0>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Generate csv files for evaluation and data evaluation in R scripts\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m outputcsv()\n",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m, in \u001b[0;36moutputcsv\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moutputcsv\u001b[39m():\n\u001b[0;32m----> 3\u001b[0m     raw_data_result_nova,lime_score_result_nova \u001b[38;5;241m=\u001b[39m result(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnova\u001b[39m\u001b[38;5;124m\"\u001b[39m,data_nova,line_data_nova,\u001b[38;5;241m5\u001b[39m) \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#raw_data_result_ironic,lime_score_result_ironic = result(2,\"ironic\",data_ironic,line_data_ironic,-4) \u001b[39;00m\n\u001b[1;32m      5\u001b[0m     raw_data_result_base,lime_score_result_base \u001b[38;5;241m=\u001b[39m result(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m,data_base,line_data_base,\u001b[38;5;241m6\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m, in \u001b[0;36mresult\u001b[0;34m(seed, projectName, data, line_data, bias)\u001b[0m\n\u001b[1;32m      7\u001b[0m correct_prediction_dict, predic_set \u001b[38;5;241m=\u001b[39m getCorrectedPredictFileDict(rf,data,test_X,test_y,divider)\n\u001b[1;32m      8\u001b[0m line_predict_dict,line_predict_set \u001b[38;5;241m=\u001b[39m getCorrectedPredictLineDict(correct_prediction_dict,line_data)\n\u001b[0;32m----> 9\u001b[0m feature_dict \u001b[38;5;241m=\u001b[39m getLimeExplainerScoreDict(projectName,correct_prediction_dict,data_count_vect,train_X,rf,seed,bias)\n\u001b[1;32m     10\u001b[0m length_count_dict,code_length_distribution,line_row_data_dict,train_row_data_dict \u001b[38;5;241m=\u001b[39m getDistributionLineInTrain(data,line_data,divider)\n\u001b[1;32m     11\u001b[0m eval_dict_ngram \u001b[38;5;241m=\u001b[39m evalLineNgram(projectName,seed)\n",
      "Cell \u001b[0;32mIn[24], line 34\u001b[0m, in \u001b[0;36mgetLimeExplainerScoreDict\u001b[0;34m(project, correct_prediction_dict, data_count_vect, lime_train_X, model, seed, bias)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(exp)\n\u001b[0;32m---> 34\u001b[0m features_val \u001b[38;5;241m=\u001b[39m exp\u001b[38;5;241m.\u001b[39mas_list(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m new_features_val \u001b[38;5;241m=\u001b[39m [tup \u001b[38;5;28;01mfor\u001b[39;00m tup \u001b[38;5;129;01min\u001b[39;00m features_val]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m feature_dict:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lime/explanation.py:141\u001b[0m, in \u001b[0;36mExplanation.as_list\u001b[0;34m(self, label, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the explanation as a list.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    given by domain_mapper. Weight is a float.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m label_to_use \u001b[38;5;241m=\u001b[39m label \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdummy_label\n\u001b[0;32m--> 141\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_mapper\u001b[38;5;241m.\u001b[39mmap_exp_ids(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_exp[label_to_use], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    142\u001b[0m ans \u001b[38;5;241m=\u001b[39m [(x[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mfloat\u001b[39m(x[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ans]\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ans\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lime/lime_tabular.py:64\u001b[0m, in \u001b[0;36mTableDomainMapper.map_exp_ids\u001b[0;34m(self, exp)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscretized_feature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscretized_feature_names\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(names[x[\u001b[38;5;241m0\u001b[39m]], x[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m exp]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lime/lime_tabular.py:64\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscretized_feature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscretized_feature_names\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(names[x[\u001b[38;5;241m0\u001b[39m]], x[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m exp]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Generate csv files for evaluation and data evaluation in R scripts\n",
    "outputcsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06155465-569b-4f75-8894-d10b224f00be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a6179-e406-421d-9ec1-76c1eb4447b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07c497-384b-4cd2-9781-ca6793323497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccdfb47-dab8-4f6c-9429-bec100ff1445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
