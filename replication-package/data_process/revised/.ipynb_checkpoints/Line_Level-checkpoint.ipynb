{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05af084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scripts information:\n",
    "#Input: File level dataset. Files: \"Nova.csv\",\"Ironic.csv\", \"Base.csv\"\n",
    "#Output: File level result. File: \"csv_revised_lineLevel_raw.csv\" includes 'dataset','changeId','fileName','lineNumber','token','ngramScore','groundTruth','lengthScore'\n",
    "#                                  \"csv_revised_limescore.csv\" includes 'dataset','changeId','fileName','lineNumber','token','limeScore'\n",
    "#Description: This script is used to generate the csv files for predicting lines to be revised for RQ2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from scipy import sparse\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import time, pickle, math, warnings, os, operator\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# path that saves prediction result from n-gram model \n",
    "eval_file_path = './dataset/eval_file'\n",
    "\n",
    "# path that saves all trained models\n",
    "model_path = './dataset/ml-model'\n",
    "\n",
    "# path that saves all LIME models\n",
    "explainer_model_path = './dataset/lime-feature-model'\n",
    "\n",
    "#load file level dataset\n",
    "data_nova = pd.read_csv('./dataset/fileLevel/Nova.csv', dtype=None, sep=',').to_numpy()\n",
    "data_ironic = pd.read_csv('./dataset/fileLevel/Ironic.csv', dtype=None, sep=',').to_numpy()\n",
    "data_base = pd.read_csv('./dataset/fileLevel/Base.csv', dtype=None, sep=',').to_numpy()\n",
    "\n",
    "#load line level dataset\n",
    "line_data_nova = pd.read_csv('./dataset/lineLevel/Nova.csv', dtype=None, sep=',').to_numpy()\n",
    "line_data_ironic = pd.read_csv('./dataset/lineLevel/Ironic.csv', dtype=None, sep=',').to_numpy()\n",
    "line_data_base = pd.read_csv('./dataset/lineLevel/Base.csv', dtype=None, sep=',').to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "794274e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate training/test datset\n",
    "def getDatasetFromRawData(data, bias):\n",
    "    row_data = data[0:,3]\n",
    "    row_data_Y = data[0:,0]\n",
    "    row_data_deletions = data[0:,6]\n",
    "    row_data_additions = data[0:,7]\n",
    "    row_data_changedLine = data[0:,8]\n",
    "  \n",
    "    Y_train = []\n",
    "    is_comment = 0\n",
    "    not_comment = 0\n",
    "    for element in row_data_Y:\n",
    "        if(element == 0):\n",
    "            Y_train.append(False)\n",
    "            not_comment += 1\n",
    "        else:\n",
    "            Y_train.append(True)\n",
    "            is_comment += 1\n",
    "    Y_train = np.array(Y_train)\n",
    "\n",
    "    #finding a index that wouldn't separate file in same changeId into both training dataset and test dataset\n",
    "    first = int(len(data)*0.6)\n",
    "    divider = int(len(data)*0.2) + first + bias\n",
    "    data_count_vect = CountVectorizer(min_df=2, max_df=0.5)\n",
    "    train_row_data = row_data[:divider]\n",
    "    test_row_data = row_data[divider:]\n",
    "    train_row_data_deletions = row_data_deletions[:divider]\n",
    "    test_row_data_deletions = row_data_deletions[divider:]\n",
    "    train_row_data_additions = row_data_additions[:divider]\n",
    "    test_row_data_additions = row_data_additions[divider:]\n",
    "    train_row_data_changedLine = row_data_changedLine[:divider]\n",
    "    test_row_data_changedLine = row_data_changedLine[divider:]\n",
    "    data_train_counts = data_count_vect.fit_transform(train_row_data)\n",
    "    data_test_counts = data_count_vect.transform(test_row_data) \n",
    "    final_train_X = np.hstack((data_train_counts.toarray(),train_row_data_deletions[:,None]))\n",
    "    final_train_X = np.hstack((final_train_X,train_row_data_additions[:,None]))\n",
    "    final_train_X = np.hstack((final_train_X,train_row_data_changedLine[:,None]))\n",
    "    final_test_X = np.hstack((data_test_counts.toarray(),test_row_data_deletions[:,None]))\n",
    "    final_test_X = np.hstack((final_test_X,test_row_data_additions[:,None]))\n",
    "    final_test_X = np.hstack((final_test_X,test_row_data_changedLine[:,None]))\n",
    "    final_train_y = Y_train[:divider]\n",
    "    final_test_y = Y_train[divider:]\n",
    "    del data_train_counts,data_test_counts,train_row_data,test_row_data,data,row_data,row_data_Y \n",
    "    return final_train_X,final_train_y,final_test_X,final_test_y,divider,data_count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55099321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResult(x, y, model):\n",
    "    print(\"AUC:\",roc_auc_score(y, model.predict_proba(x)[:,1]))\n",
    "    print(\"Precision:\",precision_score(y, model.predict(x)))\n",
    "    print(\"Recall:\",recall_score(y, model.predict(x)))\n",
    "    print(\"F1:\",f1_score(y, model.predict(x)))\n",
    "    print(\"Confusion matrix: \\n\",confusion_matrix(y, model.predict(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01a3a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRFmodel(project,rf_train_X,rf_train_y,rf_test_X,rf_test_y,seed,bias):\n",
    "    train_rf_model_path = model_path+'/RQ2_2_rf_'+project+'-'+str(seed)+str(bias)+'.pkl'\n",
    "    print(train_rf_model_path)\n",
    "    if not os.path.exists(train_rf_model_path):\n",
    "        rf = RandomForestClassifier(n_estimators=200,n_jobs=-1,random_state=seed)\n",
    "        rf_X, rf_y = SMOTE(k_neighbors=10, random_state=seed).fit_resample(rf_train_X, rf_train_y)\n",
    "        rf.fit(rf_X,rf_y)\n",
    "        rf_ouput = open(train_rf_model_path, 'wb')\n",
    "        pickle.dump(rf,rf_ouput)\n",
    "        print(\"finish to creat a new model\")\n",
    "    else:\n",
    "        with open(train_rf_model_path,'rb') as f:\n",
    "            rf = pickle.load(f)\n",
    "    printResult(rf_test_X,rf_test_y,rf)\n",
    "    return rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0744d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a dictionary include the correctly predicted files by our rf model.\n",
    "def getCorrectedPredictFileDict(model,data,test_X,test_y,divider):\n",
    "    predict_counter = 0;\n",
    "    correct_prediction_dict = {}\n",
    "    predic_set = []\n",
    "    test_dataset_row = data[divider:]\n",
    "    predict_label = model.predict(test_X)\n",
    "    for index in range(len(predict_label)):\n",
    "        if(predict_label[index] == test_y[index] and test_y[index] == 1):\n",
    "            predict_counter += 1\n",
    "            changeId = test_dataset_row[index][2]\n",
    "            fileName = test_dataset_row[index][1]\n",
    "            if changeId in correct_prediction_dict:\n",
    "                correct_prediction_dict[changeId][fileName] = test_X[index]\n",
    "            else:\n",
    "                correct_prediction_dict[changeId] = {fileName:test_X[index]}\n",
    "            predic_set.append((changeId,fileName))\n",
    "    return correct_prediction_dict,set(predic_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcc522f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a dictionary include all lines in the correctly predicted files.\n",
    "def getCorrectedPredictLineDict(correct_prediction_dict,line_data, ngram=False):\n",
    "    line_check_counter = 0\n",
    "    line_predict_dict = {}\n",
    "    line_predict_set = []\n",
    "    for line_element in line_data:\n",
    "        line_predict_fileName = line_element[1]\n",
    "        line_predict_changeId = line_element[2]\n",
    "        if ngram == True:\n",
    "            line_predict_code = line_element[3].replace(\"<NUMBER>\",\"\")\n",
    "        else:\n",
    "            line_predict_code = line_element[3]\n",
    "        line_predict_loc = line_element[10]\n",
    "        line_predict_label = line_element[0]\n",
    "        if line_predict_changeId in correct_prediction_dict:\n",
    "            if line_predict_fileName in correct_prediction_dict[line_predict_changeId]:\n",
    "                if line_predict_changeId in line_predict_dict:\n",
    "                    if line_predict_fileName in line_predict_dict[line_predict_changeId]:\n",
    "                        line_predict_dict[line_predict_changeId][line_predict_fileName].append((line_predict_code, line_predict_loc, line_predict_label))\n",
    "                    else:\n",
    "                        line_predict_dict[line_predict_changeId][line_predict_fileName] = [(line_predict_code, line_predict_loc, line_predict_label)]\n",
    "                else:\n",
    "                    line_predict_dict[line_predict_changeId] = {line_predict_fileName:[(line_predict_code, line_predict_loc, line_predict_label)]}\n",
    "                line_predict_set.append((line_predict_changeId,line_predict_fileName))\n",
    "    return line_predict_dict,set(line_predict_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d2ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistributionLineInTrain(row_data, line_row_data,divider):\n",
    "    train_row_data_contents = row_data[:divider]\n",
    "    train_row_data_dict = {}\n",
    "    #Build a dict only includes the training data.\n",
    "    for train_row_data_content in train_row_data_contents:\n",
    "        train_row_data_label = train_row_data_content[0]\n",
    "        train_row_data_file = train_row_data_content[1]\n",
    "        train_row_data_changeId = train_row_data_content[2]\n",
    "        if train_row_data_label == 1:\n",
    "            if train_row_data_changeId in train_row_data_dict:\n",
    "                train_row_data_dict[train_row_data_changeId].append(train_row_data_file)\n",
    "            else:\n",
    "                train_row_data_dict[train_row_data_changeId] = [train_row_data_file]\n",
    "    line_row_data_dict = {}\n",
    "    code_length_distribution = []\n",
    "    for line_code_content in line_row_data:\n",
    "        train_row_line_data_label = line_code_content[0]\n",
    "        train_row_line_data_file = line_code_content[1]\n",
    "        train_row_line_data_changeId = line_code_content[2]\n",
    "        train_row_line_data_code = line_code_content[3]\n",
    "        #check if the LOC's file and change id in the training dataset \n",
    "        if train_row_line_data_changeId in train_row_data_dict and train_row_line_data_file in train_row_data_dict[train_row_line_data_changeId] and train_row_line_data_label == 1:\n",
    "            #Save to a new dict\n",
    "            code_length_distribution.append(len(train_row_line_data_code.split()))\n",
    "            if train_row_line_data_changeId in line_row_data_dict:\n",
    "                if train_row_line_data_file in line_row_data_dict[train_row_line_data_changeId]:\n",
    "                    line_row_data_dict[train_row_line_data_changeId][train_row_line_data_file].append(train_row_line_data_code)\n",
    "                else:\n",
    "                    line_row_data_dict[train_row_line_data_changeId] = {train_row_line_data_file:[train_row_line_data_code]}\n",
    "            else:\n",
    "                line_row_data_dict[train_row_line_data_changeId] = {train_row_line_data_file:[train_row_line_data_code]}\n",
    "    length_count_dict = {}\n",
    "    (unique, counts) = np.unique(code_length_distribution,return_counts=True)\n",
    "    for index in range(len(unique)):\n",
    "        length_count_dict[unique[index]] = float(counts[index]/len(code_length_distribution))\n",
    "    return length_count_dict,code_length_distribution,line_row_data_dict,train_row_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "700bfa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the LIME score \n",
    "def getLimeExplainerScoreDict(project,correct_prediction_dict,data_count_vect,lime_train_X,model,seed,bias):\n",
    "    explainer_feature_path = explainer_model_path + '/feature_ouput_' + project +'-'+ str(seed)+'-'+str(bias)+'.pkl' \n",
    "\n",
    "    if not os.path.exists(explainer_feature_path): \n",
    "        feature_dict = {}\n",
    "                \n",
    "        stop_tokens = [\"*deletedLine\",\"*addedLine\",'*changedLine']\n",
    "        python_common_tokens = []\n",
    "        stop_tokens = [\"*deletedLine\",\"*addedLine\",\"*changedLine\",\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "        python_common_tokens = ['abs','delattr','hash','memoryview','set','all','dict','help','min','setattr','any','dir','hex','next','slice','ascii','divmod','id','object','sorted','bin','enumerate','input','oct','staticmethod','bool','eval','int','open','str','breakpoint','exec','isinstance','ord','sum','bytearray','filter','issubclass','pow','super','bytes','float','iter','print','tuple','callable','format','len','property','type','chr','frozenset','list','range','vars','classmethod','getattr','locals','repr','zip','compile','globals','map','reversed','import','complex','hasattr','max','round','False','await','else','import','passNone','break','except','in','raise','True','class','finally','is','return','and','continue','for','lambda','try','as','def','from','nonlocal','while','assert','del','global','not','with','async','elif','if','or','yield', 'self']\n",
    "        c_common_tokens = ['auto','const','double','float','int','short','struct','unsigned','break','continue','else','for','long','signed','switch','void','case','default','enum','goto','register','sizeof','typedef','volatile','char','do','extern','if','return','static','union','while','asm','namespace','try','bool','explicit','new','typeid','catch','false','operator','template','typename','class','friend','private','this','using','inline','public','throw','virtual','delete','mutable','protected','true']\n",
    "        if project == 'nova' or project == 'ironic':\n",
    "            print(\"using python stopword\")\n",
    "            common_tokens = python_common_tokens\n",
    "        if project == 'base':\n",
    "            print(\"using c stopword\")\n",
    "            common_tokens = c_common_tokens\n",
    "        all_features = data_count_vect.get_feature_names() + ['*deletedLine','*addedLine','*changedLine']\n",
    "        explainer = LimeTabularExplainer(lime_train_X, \n",
    "                                          feature_names=all_features, \n",
    "                                          class_names=['True','False'],\n",
    "                                          discretize_continuous=False, random_state=seed\n",
    "                                         )\n",
    "        feature_training_counter = 0\n",
    "        print(\"Training explainer\")\n",
    "        for key,value in correct_prediction_dict.items():\n",
    "            for fileName, codeArray in value.items():\n",
    "                print(feature_training_counter)\n",
    "                exp = explainer.explain_instance(codeArray,model.predict_proba,num_features=len(all_features), top_labels=1)\n",
    "                features_val = exp.as_list(label=1)\n",
    "                new_features_val = [tup for tup in features_val]\n",
    "                if key in feature_dict:\n",
    "                    feature_dict[key][fileName] = {val[0]:val[1] for val in new_features_val if val[0] not in stop_tokens and val[0] not in common_tokens}\n",
    "                else:\n",
    "                    feature_dict[key] = {fileName: {val[0]:val[1] for val in new_features_val if val[0] not in stop_tokens and val[0] not in common_tokens}}\n",
    "                feature_training_counter += 1\n",
    "                \n",
    "        feature_ouput = open(explainer_feature_path, 'wb')\n",
    "        pickle.dump(feature_dict,feature_ouput)\n",
    "        \n",
    "        print(\"write feature output to pickle\")\n",
    "    else:\n",
    "        with open(explainer_feature_path, \"rb\") as f:\n",
    "            feature_dict = pickle.load(f)\n",
    "    return feature_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ba315de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ngram\n",
    "# 1. Run functions\"outputSlpTestFile\",\"outputSlpTrainFile\",\"generateNgramFiles\" \n",
    "# 2. A training file \"ngram_train_PORJECTNAME_2.csv\" and a testing file \"ngram_test_PORJECTNAME_2.csv\" for ngram approach will be geenrated in the path \"data_process/revised/dataset/eval_file/\"\n",
    "# 2. Put these two files to the path \"/SLP-Core/src/main/java/slp/core/example/\" and follow the code comments in EntrpoyForEachLine.java. It would generate result file for ngram named \"entropy_PROJECT_2.csv\" \n",
    "# 3. Then put result of ngram file \"entropy_PROJECT_2.csv\" to the path \"data_process/revised/dataset/eval_file/\"\n",
    "# 4. Run result code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73e5187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate test dataset for ngram baseline approach\n",
    "#Output files \"ngram_test_PORJECTNAME_2.csv\"\n",
    "def outputSlpTestFile(project,line_predict_dict_ngram,seed):\n",
    "    csv_ngram_path = eval_file_path + '/ngram_test_revised_'+ project +'-'+str(seed)+'.csv'\n",
    "    csv_lime_path = eval_file_path + '/lime_test_'+ project +'-'+str(seed)+'.csv'\n",
    "    csv_ngram_file = open(csv_ngram_path,\"w\")\n",
    "    csv_lime_file = open(csv_lime_path,\"w\")\n",
    "    csv_ngram_writer = csv.writer(csv_ngram_file,quoting=csv.QUOTE_NONE,escapechar='') \n",
    "    csv_lime_writer = csv.writer(csv_lime_file) \n",
    "    for key,value in line_predict_dict_ngram.items():\n",
    "        for fileName, codeLines in value.items():\n",
    "            for codeLine in codeLines:\n",
    "                csv_ngram_writer.writerow([codeLine[0]])\n",
    "                csv_lime_writer.writerow([key, fileName, codeLine[0].strip(),codeLine[1],codeLine[2]])\n",
    "    csv_ngram_file.close()  \n",
    "    csv_lime_file.close()\n",
    "    print(\"write test datset done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c7f0ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate training dataset for ngram baseline approach\n",
    "#Output files \"ngram_train_revised_PORJECTNAME_2.csv\"\n",
    "def outputSlpTrainFile(project,seed):\n",
    "    capitalied_project = project.capitalize()\n",
    "    data_ngram_train = pd.read_csv(eval_file_path + '/ngramTrain' + capitalied_project + '.csv', dtype=None, sep=',',header=None).to_numpy().tolist()\n",
    "    data_train_ngram_test = pd.read_csv(eval_file_path + '/ngram_test_revised_' + project + '-'+str(seed)+'.csv', dtype=None, sep=',',header=None).to_numpy().tolist()\n",
    "    data_ngram_train = [x[0] for x in data_ngram_train]\n",
    "    data_train_ngram_test = [x[0].strip() for x in data_train_ngram_test]\n",
    "    new_data_train_ngram_test = list(set(data_ngram_train) - set(data_train_ngram_test))\n",
    "\n",
    "    csv_ngram_train_path = eval_file_path + '/ngram_train_revised_'+ project +'-'+str(seed)+'.csv'\n",
    "    csv_new_train_ngram_file = open(csv_ngram_train_path,\"w\")\n",
    "    csv_ngram_train_writer = csv.writer(csv_new_train_ngram_file,quoting=csv.QUOTE_NONE,escapechar='') \n",
    "    for code in new_data_train_ngram_test:\n",
    "        csv_ngram_train_writer.writerow([code])\n",
    "    csv_new_train_ngram_file.close()  \n",
    "    print(\"write train datset done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12dd9823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output ngram training/test dataset\n",
    "def generateNgramFiles(seed,projectName,data,line_data,bias):\n",
    "    projectName = projectName\n",
    "    train_X, train_y, test_X, test_y,divider,data_count_vect = getDatasetFromRawData(data, bias)\n",
    "    rf = trainRFmodel(projectName,train_X, train_y, test_X, test_y,seed,bias)\n",
    "    correct_prediction_dict, predic_set = getCorrectedPredictFileDict(rf,data,test_X,test_y,divider)\n",
    "    line_predict_dict_ngram,line_predict_set_ngram = getCorrectedPredictLineDict(correct_prediction_dict,line_data, ngram=True)\n",
    "    outputSlpTestFile(projectName,line_predict_dict_ngram,seed)\n",
    "    outputSlpTrainFile(projectName,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70065a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate ngram rank score\n",
    "def evalLineNgram(project,seed):\n",
    "    data_entropy = pd.read_csv(eval_file_path + '/entropy_revised_'+project+'-'+str(seed)+'.csv', dtype=None, sep=',',header=None).to_numpy()\n",
    "    data_ngram_test = pd.read_csv(eval_file_path + '/lime_test_'+project+'-'+str(seed)+'.csv', dtype=None, sep=',',header=None).to_numpy()\n",
    "\n",
    "    ngram_eval_line = {}\n",
    "    for index in range(len(data_ngram_test)):\n",
    "        eval_line_ngram_changeId = data_ngram_test[index][0]\n",
    "        eval_line_ngram_fileName = data_ngram_test[index][1]\n",
    "        eval_line_ngram_code = data_ngram_test[index][2]\n",
    "        eval_line_ngram_line = data_ngram_test[index][3]\n",
    "        eval_line_ngram_label = data_ngram_test[index][4]\n",
    "        eval_line_entropy = data_entropy[index][0]\n",
    "        if eval_line_ngram_changeId in ngram_eval_line:\n",
    "            if eval_line_ngram_fileName in ngram_eval_line[eval_line_ngram_changeId]:\n",
    "                ngram_eval_line[eval_line_ngram_changeId][eval_line_ngram_fileName].append((eval_line_ngram_code,eval_line_ngram_line,eval_line_ngram_label,eval_line_entropy))\n",
    "            else:\n",
    "                ngram_eval_line[eval_line_ngram_changeId][eval_line_ngram_fileName] = [(eval_line_ngram_code,eval_line_ngram_line,eval_line_ngram_label,eval_line_entropy)]\n",
    "        else:\n",
    "            ngram_eval_line[eval_line_ngram_changeId] = {eval_line_ngram_fileName:[(eval_line_ngram_code,eval_line_ngram_line,eval_line_ngram_label,eval_line_entropy)]}\n",
    "    return ngram_eval_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5d1638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate csv files fr our approach\n",
    "def generateRawDataCsv(result,project,eval_dict_ngram,length_count_dict):\n",
    "    for changeId, pair in eval_dict_ngram.items():\n",
    "        for fileName, value in pair.items():\n",
    "            for index, element in enumerate(value):\n",
    "                token_length = len(str(element[0]).split())\n",
    "                length_score = length_count_dict.get(token_length,0)\n",
    "                result.append({\"dataset\": project, \"changeId\":changeId, \"fileName\": fileName, \"lineNumber\":element[1], \"token\":element[0], \"ngramScore\":str(element[3]), \"groundTruth\":element[2], \"lengthScore\":length_score})\n",
    "    return result\n",
    "\n",
    "def generateLimeScoreDataCsv(result,project,feature_dict,line_predict_dict):\n",
    "    for changeId, pair in line_predict_dict.items():\n",
    "        for fileName, value in pair.items():\n",
    "            for index, element in enumerate(value):\n",
    "                code = list(set(element[0].lower().split()))\n",
    "                for token in code:\n",
    "                    token_score = feature_dict[changeId][fileName].get(token,0)\n",
    "                    result.append({\"dataset\": project, \"changeId\":changeId, \"fileName\": fileName, \"lineNumber\":element[1],\"token\":token,\"limeScore\":token_score})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d97b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate result for our approach\n",
    "def result(seed,projectName,data,line_data,bias):\n",
    "    projectName = projectName\n",
    "    #RF\n",
    "    train_X, train_y, test_X, test_y,divider,data_count_vect = getDatasetFromRawData(data, bias)\n",
    "    rf = trainRFmodel(projectName,train_X, train_y, test_X, test_y,seed,bias)\n",
    "    correct_prediction_dict, predic_set = getCorrectedPredictFileDict(rf,data,test_X,test_y,divider)\n",
    "    line_predict_dict,line_predict_set = getCorrectedPredictLineDict(correct_prediction_dict,line_data)\n",
    "    feature_dict = getLimeExplainerScoreDict(projectName,correct_prediction_dict,data_count_vect,train_X,rf,seed,bias)\n",
    "    length_count_dict,code_length_distribution,line_row_data_dict,train_row_data_dict = getDistributionLineInTrain(data,line_data,divider)\n",
    "    eval_dict_ngram = evalLineNgram(projectName,seed)\n",
    "    raw_data_result = []\n",
    "    lime_score_result = []\n",
    "    raw_data_result = generateRawDataCsv(raw_data_result,projectName,eval_dict_ngram,length_count_dict)\n",
    "    lime_score_result = generateLimeScoreDataCsv(lime_score_result,projectName,feature_dict,line_predict_dict)\n",
    "    return raw_data_result,lime_score_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72bae44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output csv files at path:./dataset/csv/\n",
    "def outputcsv():\n",
    "    raw_data_result_nova,lime_score_result_nova = result(2,\"nova\",data_nova,line_data_nova,5) \n",
    "    raw_data_result_ironic,lime_score_result_ironic = result(2,\"ironic\",data_ironic,line_data_ironic,-4) \n",
    "    raw_data_result_base,lime_score_result_base = result(2,\"base\",data_base,line_data_base,6)\n",
    "    raw_result = raw_data_result_nova + raw_data_result_ironic + raw_data_result_base\n",
    "    lime_result = lime_score_result_nova + lime_score_result_ironic + lime_score_result_base\n",
    "    \n",
    "    csv_path_raw = './dataset/csv/csv_revised_lineLevel_raw.csv'\n",
    "    csv_file_raw = open(csv_path_raw,\"w\")\n",
    "    fieldnames_raw = ['dataset','changeId','fileName','lineNumber','token','ngramScore','groundTruth','lengthScore']\n",
    "    print(\"generating csv, length:\", len(raw_result))\n",
    "    csv_writer_raw = csv.DictWriter(csv_file_raw,quoting=csv.QUOTE_NONE,escapechar=';', fieldnames= fieldnames_raw) \n",
    "    csv_writer_raw.writeheader()\n",
    "    for row in raw_result:\n",
    "        csv_writer_raw.writerow(row) \n",
    "    print(\"csv generated\")\n",
    "       \n",
    "    csv_path_raw = './dataset/csv/csv_revised_limescore.csv'\n",
    "    csv_file_raw = open(csv_path_raw,\"w\")\n",
    "    fieldnames_raw = ['dataset','changeId','fileName','lineNumber','token','limeScore']\n",
    "    print(\"generating csv, length:\", len(raw_result))\n",
    "    csv_writer_raw = csv.DictWriter(csv_file_raw,quoting=csv.QUOTE_NONE,escapechar=';', fieldnames= fieldnames_raw) \n",
    "    csv_writer_raw.writeheader()\n",
    "    for row in lime_result:\n",
    "        csv_writer_raw.writerow(row) \n",
    "    print(\"csv generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e36e82f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/ml-model/RQ2_2_rf_nova-25.pkl\n",
      "AUC: 0.7674421270819577\n",
      "Precision: 0.5156695156695157\n",
      "Recall: 0.40401785714285715\n",
      "F1: 0.4530663329161451\n",
      "Confusion matrix: \n",
      " [[1777  170]\n",
      " [ 267  181]]\n",
      "write test datset done\n",
      "write train datset done\n",
      "./dataset/ml-model/RQ2_2_rf_ironic-2-4.pkl\n",
      "AUC: 0.6779478789295312\n",
      "Precision: 0.3813953488372093\n",
      "Recall: 0.4120603015075377\n",
      "F1: 0.39613526570048313\n",
      "Confusion matrix: \n",
      " [[512 133]\n",
      " [117  82]]\n",
      "write test datset done\n",
      "write train datset done\n",
      "./dataset/ml-model/RQ2_2_rf_base-26.pkl\n",
      "AUC: 0.6894346821248506\n",
      "Precision: 0.46537396121883656\n",
      "Recall: 0.23076923076923078\n",
      "F1: 0.3085399449035813\n",
      "Confusion matrix: \n",
      " [[3133  193]\n",
      " [ 560  168]]\n",
      "write test datset done\n",
      "write train datset done\n"
     ]
    }
   ],
   "source": [
    "#Generate ngram training/test dataset for studied projects\n",
    "generateNgramFiles(2,\"nova\",data_nova,line_data_nova,5)\n",
    "generateNgramFiles(2,\"ironic\",data_ironic,line_data_ironic,-4)\n",
    "generateNgramFiles(2,\"base\",data_base,line_data_base,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8563bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/ml-model/RQ2_2_rf_nova-25.pkl\n",
      "AUC: 0.7674404074033312\n",
      "Precision: 0.5156695156695157\n",
      "Recall: 0.40401785714285715\n",
      "F1: 0.4530663329161451\n",
      "Confusion matrix: \n",
      " [[1777  170]\n",
      " [ 267  181]]\n",
      "./dataset/ml-model/RQ2_2_rf_ironic-2-4.pkl\n",
      "AUC: 0.6779478789295313\n",
      "Precision: 0.3813953488372093\n",
      "Recall: 0.4120603015075377\n",
      "F1: 0.39613526570048313\n",
      "Confusion matrix: \n",
      " [[512 133]\n",
      " [117  82]]\n",
      "./dataset/ml-model/RQ2_2_rf_base-26.pkl\n",
      "AUC: 0.6894425290584341\n",
      "Precision: 0.46537396121883656\n",
      "Recall: 0.23076923076923078\n",
      "F1: 0.3085399449035813\n",
      "Confusion matrix: \n",
      " [[3133  193]\n",
      " [ 560  168]]\n",
      "generating csv, length: 25496\n",
      "csv generated\n",
      "generating csv, length: 25496\n",
      "csv generated\n"
     ]
    }
   ],
   "source": [
    "#Generate csv files for evaluation and data evaluation in R scripts\n",
    "outputcsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0d66d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
